{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SG6_T.ipynb","provenance":[{"file_id":"1nZlFsbfRICzIzzaF7xJRME88wvs6o6-R","timestamp":1619748735131},{"file_id":"1dG1f9f1YsC4C9V92S-fcpvmqh21etlxY","timestamp":1619742241908},{"file_id":"13TzmybVpQsJNM49Ydx2Jz2tqFdHRil-P","timestamp":1619740498955},{"file_id":"1g3CxbhDn042askx1LuvPQJ8vTedyC3gB","timestamp":1619670717561},{"file_id":"107uPINN997VHutBwe2VQn6OfstRjI66o","timestamp":1619656872334},{"file_id":"https://github.com/dvschultz/stylegan2-ada-pytorch/blob/main/SG2_ADA_PyTorch.ipynb","timestamp":1619611090239}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jG7ZEc_982io"},"source":["# StyleGAN2-ADA-PyTorch\n","\n","**Notes**\n","* Training section should be fairly stable. I’ll slowly add features but it should work for most mainstream use cases\n","* Inference section is a work in progress. If you come across bug or have feature requests please post them in [Slack](https://ml-images.slack.com/archives/CLJGF384R) or on [Github](https://github.com/dvschultz/stylegan2-ada-pytorch/issues)\n","\n","---\n","\n","If you find this notebook useful, consider signing up for my [Patreon](https://www.patreon.com/bustbright) or [YouTube channel](https://www.youtube.com/channel/UCaZuPdmZ380SFUMKHVsv_AA/join). You can also send me a one-time payment on [Venmo](https://venmo.com/Derrick-Schultz)."]},{"cell_type":"markdown","metadata":{"id":"Vj4PG4_i9Alt"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"qGEXPcFJ9UTY"},"source":["Let’s start by checking to see what GPU we’ve been assigned. Ideally we get a V100, but a P100 is fine too. Other GPUs may lead to issues."]},{"cell_type":"code","metadata":{"id":"9giNAy_VqBku","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619747859060,"user_tz":-540,"elapsed":2650,"user":{"displayName":"Pado파도","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK5icpssdxim4CNM813_k0CxYcl_syRlrFhRSc=s64","userId":"07447657114463313189"}},"outputId":"41b08177-d2a8-4252-e17f-e5ea63cefe47"},"source":["!nvidia-smi "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fri Apr 30 01:57:35 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_zedqBUQXdim","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619616474945,"user_tz":-540,"elapsed":911,"user":{"displayName":"SHOT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5vvPwI0wxV86goecWvaz89UBqVzkx1WnnD5ovSU8=s64","userId":"16892596591524153853"}},"outputId":"9db01dc6-e5f8-4994-db2d-f1a0712a209f"},"source":["!ls \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["colab-sg2-ada-pytorch  sample_data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rSV_HEoD9dxo"},"source":["Next let’s connect our Google Drive account. This is optional but highly recommended."]},{"cell_type":"code","metadata":{"id":"IuVPuJmbigRs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619747957264,"user_tz":-540,"elapsed":57748,"user":{"displayName":"Pado파도","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK5icpssdxim4CNM813_k0CxYcl_syRlrFhRSc=s64","userId":"07447657114463313189"}},"outputId":"a45a85d0-f957-4c12-a21f-e611a28f7711"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z2_B1aKxX2Eg"},"source":["!ls '/content/drive/My Drive'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c_JZrWZcEjXD","executionInfo":{"status":"ok","timestamp":1619747961660,"user_tz":-540,"elapsed":957,"user":{"displayName":"Pado파도","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK5icpssdxim4CNM813_k0CxYcl_syRlrFhRSc=s64","userId":"07447657114463313189"}},"outputId":"21a22b88-d7e1-45ce-d337-ab7840bacb51"},"source":["!pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nTjVmfSK9CYa"},"source":["## Install repo\n","\n","The next cell will install the StyleGAN repository in Google Drive. If you have already installed it it will just move into that folder. If you don’t have Google Drive connected it will just install the necessary code in Colab."]},{"cell_type":"code","metadata":{"id":"B8ADVNpBh8Ox","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619756911129,"user_tz":-540,"elapsed":6100,"user":{"displayName":"Pado파도","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK5icpssdxim4CNM813_k0CxYcl_syRlrFhRSc=s64","userId":"07447657114463313189"}},"outputId":"38d21b8f-13eb-4070-f2da-14e0aaf3c374"},"source":["\n","!mkdir colab-sg2-ada-pytorch\n","%cd colab-sg2-ada-pytorch\n","!git clone https://github.com/cordob/stylegan2-ada-pytorch\n","%cd stylegan2-ada-pytorch\n","!mkdir downloads\n","!mkdir datasets\n","!mkdir pretrained\n","!mkdir training_runs\n","\n","\n","!pip install ninja opensimplex"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/colab-sg2-ada-pytorch\n","Cloning into 'stylegan2-ada-pytorch'...\n","remote: Enumerating objects: 188, done.\u001b[K\n","remote: Counting objects: 100% (63/63), done.\u001b[K\n","remote: Compressing objects: 100% (43/43), done.\u001b[K\n","remote: Total 188 (delta 36), reused 37 (delta 20), pack-reused 125\u001b[K\n","Receiving objects: 100% (188/188), 2.39 MiB | 3.73 MiB/s, done.\n","Resolving deltas: 100% (91/91), done.\n","/content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\n","Collecting ninja\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/de/393468f2a37fc2c1dc3a06afc37775e27fde2d16845424141d4da62c686d/ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107kB)\n","\u001b[K     |████████████████████████████████| 112kB 13.4MB/s \n","\u001b[?25hCollecting opensimplex\n","  Downloading https://files.pythonhosted.org/packages/9c/ad/9b758f9ff9dcd23fc574bb3aa1de844adb1179c9be9711e9f798614d4b2f/opensimplex-0.3-py3-none-any.whl\n","Installing collected packages: ninja, opensimplex\n","Successfully installed ninja-1.10.0.post2 opensimplex-0.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-cyV-0x2etdf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619672310862,"user_tz":-540,"elapsed":533,"user":{"displayName":"SHOT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5vvPwI0wxV86goecWvaz89UBqVzkx1WnnD5ovSU8=s64","userId":"16892596591524153853"}},"outputId":"4bda0037-2eb3-4e9d-ad65-2ce17d3490f4"},"source":["!pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/colab-sg2-ada-pytorch/colab-sg2-ada-pytorch/stylegan2ada\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fYTc-YQ8hWGk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619756915415,"user_tz":-540,"elapsed":844,"user":{"displayName":"Pado파도","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK5icpssdxim4CNM813_k0CxYcl_syRlrFhRSc=s64","userId":"07447657114463313189"}},"outputId":"fd4a6366-c39c-48f8-ef92-2430f08abc46"},"source":["!ls"],"execution_count":4,"outputs":[{"output_type":"stream","text":["calc_metrics.py  docs\t      projector.py\t  torch_utils\n","cmd\t\t downloads    README.md\t\t  training\n","datasets\t generate.py  render.py\t\t  training_runs\n","dataset_tool.py  legacy.py    SG2_T2_colab.ipynb  train.py\n","dnnlib\t\t LICENSE.txt  SG2_T_colab.ipynb\n","Dockerfile\t metrics      SG3_T.ipynb\n","docker_run.sh\t pretrained   style_mixing.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6qYvmln-_3b8","executionInfo":{"status":"ok","timestamp":1619750015398,"user_tz":-540,"elapsed":20215,"user":{"displayName":"Pado파도","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK5icpssdxim4CNM813_k0CxYcl_syRlrFhRSc=s64","userId":"07447657114463313189"}},"outputId":"7376cd64-036b-48a6-becf-47e9fc99de3f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JhHwUgMQ_7Xv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619750215312,"user_tz":-540,"elapsed":447,"user":{"displayName":"Pado파도","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK5icpssdxim4CNM813_k0CxYcl_syRlrFhRSc=s64","userId":"07447657114463313189"}},"outputId":"77b46ce9-ab0e-482d-be9a-0a4d531efadb"},"source":["!ls '/content/drive/My Drive'\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" 북유럽1.wav\t\t    북유럽2.wav        MUNCH.zip\n","'2020-11-04 19-09-29.mov'  'Colab Notebooks'   wikiart.pkl\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zMl-oOoe64it"},"source":["generate images from prtained\n"]},{"cell_type":"code","metadata":{"id":"wwLlE-wc3DJI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619748056912,"user_tz":-540,"elapsed":71877,"user":{"displayName":"Pado파도","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK5icpssdxim4CNM813_k0CxYcl_syRlrFhRSc=s64","userId":"07447657114463313189"}},"outputId":"5578d074-6e94-4e6b-ffc1-d48e3815826f"},"source":["!python generate.py --outdir=out --trunc=0.7 --seeds=8100-8125 --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading networks from \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\"...\n","Downloading https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl ... done\n","Generating image for seed 8100 (0/26) ...\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n","Generating image for seed 8101 (1/26) ...\n","Generating image for seed 8102 (2/26) ...\n","Generating image for seed 8103 (3/26) ...\n","Generating image for seed 8104 (4/26) ...\n","Generating image for seed 8105 (5/26) ...\n","Generating image for seed 8106 (6/26) ...\n","Generating image for seed 8107 (7/26) ...\n","Generating image for seed 8108 (8/26) ...\n","Generating image for seed 8109 (9/26) ...\n","Generating image for seed 8110 (10/26) ...\n","Generating image for seed 8111 (11/26) ...\n","Generating image for seed 8112 (12/26) ...\n","Generating image for seed 8113 (13/26) ...\n","Generating image for seed 8114 (14/26) ...\n","Generating image for seed 8115 (15/26) ...\n","Generating image for seed 8116 (16/26) ...\n","Generating image for seed 8117 (17/26) ...\n","Generating image for seed 8118 (18/26) ...\n","Generating image for seed 8119 (19/26) ...\n","Generating image for seed 8120 (20/26) ...\n","Generating image for seed 8121 (21/26) ...\n","Generating image for seed 8122 (22/26) ...\n","Generating image for seed 8123 (23/26) ...\n","Generating image for seed 8124 (24/26) ...\n","Generating image for seed 8125 (25/26) ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Frahw_kxfLhr"},"source":["!sudo apt install imagemagick-6.q16"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9en6vtSeqFD"},"source":["\n","\n","!montage -mode concatenate -tile 4x4 out/*.png out/result.jpg\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jMiV3ga97UkH"},"source":["!ls out\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PrIFWaXz8cah"},"source":["from google.colab import files\n","from IPython import display\n","display.Image(\"out/result-0.jpg\",\n","              width=1600)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"csqA7SOj-Joe"},"source":["!ffmpeg -framerate 2 -pattern_type glob -i 'out/*.png' \\\n","  -c:v libx264 out3.mp4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"BWcWH4xgDmaF","executionInfo":{"status":"ok","timestamp":1619656758594,"user_tz":-540,"elapsed":823,"user":{"displayName":"SHOT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5vvPwI0wxV86goecWvaz89UBqVzkx1WnnD5ovSU8=s64","userId":"16892596591524153853"}},"outputId":"dc1a6716-d10f-4342-daf2-9691b55ec8f9"},"source":["from google.colab import files\n","files.download('out3.mp4')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_08e7b428-ceb2-4870-b698-0bd9004882a9\", \"out3.mp4\", 4223857)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AUsf_JY42kxW","executionInfo":{"status":"ok","timestamp":1619653909635,"user_tz":-540,"elapsed":29873,"user":{"displayName":"SHOT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5vvPwI0wxV86goecWvaz89UBqVzkx1WnnD5ovSU8=s64","userId":"16892596591524153853"}},"outputId":"7896f7b9-114d-4294-9075-657b7be16881"},"source":["!python render.py --mp4_fps 30 --filename test --duration_sec 5 --network_pkl  https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading networks from \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\"...\n","Generating latent vectors...\n","[150, 1, 512]\n","Rendering...\n","truncation_psi = 1 , noise_mode = none\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-7rZnvjU55J7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619757178611,"user_tz":-540,"elapsed":642,"user":{"displayName":"Pado파도","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK5icpssdxim4CNM813_k0CxYcl_syRlrFhRSc=s64","userId":"07447657114463313189"}},"outputId":"ab3ed87c-5962-4f0b-9bcf-2e8db54bb8e7"},"source":["!pwd\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["/content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YCaJJNjf6whX"},"source":["video download \n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"-61iJwqs5sBT","executionInfo":{"status":"ok","timestamp":1619654092842,"user_tz":-540,"elapsed":461,"user":{"displayName":"SHOT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5vvPwI0wxV86goecWvaz89UBqVzkx1WnnD5ovSU8=s64","userId":"16892596591524153853"}},"outputId":"acad5061-b035-4a0f-e739-8bad7fdc90c0"},"source":["from google.colab import files\n","files.download('videos/seed1619653880.mp4')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_7eed9908-2ee2-433e-bb8c-3880089eb5df\", \"seed1619653880.mp4\", 9808696)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"cK0ESEo06L3h"},"source":["!rm videos/*.*"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9jMmUpn4DWRe"},"source":["local file upload !!"]},{"cell_type":"code","metadata":{"id":"OzAYBwowYMi8","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":76},"executionInfo":{"status":"ok","timestamp":1619757138889,"user_tz":-540,"elapsed":67596,"user":{"displayName":"Pado파도","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK5icpssdxim4CNM813_k0CxYcl_syRlrFhRSc=s64","userId":"07447657114463313189"}},"outputId":"6ad488e0-0215-4eff-b774-35191b704932"},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-65c87191-7380-4be6-b51a-6509105b0808\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-65c87191-7380-4be6-b51a-6509105b0808\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving MUNCH.zip to MUNCH.zip\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-qLCNbV9XMrc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619757162851,"user_tz":-540,"elapsed":679,"user":{"displayName":"Pado파도","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK5icpssdxim4CNM813_k0CxYcl_syRlrFhRSc=s64","userId":"07447657114463313189"}},"outputId":"2db05aee-d1b0-4c87-99f4-560911f68489"},"source":["!ls '/content/drive/My Drive/MUNCH.zip' "],"execution_count":12,"outputs":[{"output_type":"stream","text":["ls: cannot access '/content/drive/My Drive/MUNCH.zip': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u09225Mao0cb","executionInfo":{"status":"ok","timestamp":1619757170177,"user_tz":-540,"elapsed":602,"user":{"displayName":"Pado파도","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK5icpssdxim4CNM813_k0CxYcl_syRlrFhRSc=s64","userId":"07447657114463313189"}},"outputId":"17f0d522-243f-4c60-d056-dafe39632190"},"source":["!pwd"],"execution_count":13,"outputs":[{"output_type":"stream","text":["/content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5zNKwafgzHtw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619757226610,"user_tz":-540,"elapsed":817,"user":{"displayName":"Pado파도","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK5icpssdxim4CNM813_k0CxYcl_syRlrFhRSc=s64","userId":"07447657114463313189"}},"outputId":"e6614521-e771-4100-dbc9-f8a08f48ec31"},"source":["!unzip 'MUNCH.zip' -d /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Archive:  MUNCH.zip\n","   creating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/\n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_1.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_10.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_11.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_13.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_14.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_15.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_16.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_17.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_18.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_19.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_2.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_20.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_21.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_22.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_23.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_24.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_25.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_26.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_27.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_28.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_29.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_3.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_30.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_31.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_32.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_33.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_34.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_35.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_36.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_37.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_38.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_39.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_4.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_40.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_41.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_42.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_43.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_44.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_45.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_46.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_47.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_48.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_49.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_5.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_50.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_51.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_52.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_53.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_54.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_55.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_56.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_57.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_58.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_59.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_6.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_60.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_61.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_62.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_63.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_64.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_65.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_66.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_67.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_7.jpg  \n","  inflating: /content/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/m3/Edvard_Munch_8.jpg  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qd_bYsurYxTM"},"source":["!ls m3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r2NmSQzRjoXU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619757191489,"user_tz":-540,"elapsed":837,"user":{"displayName":"Pado파도","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK5icpssdxim4CNM813_k0CxYcl_syRlrFhRSc=s64","userId":"07447657114463313189"}},"outputId":"48518e5e-7a6a-4bb4-f037-eb73ffb72136"},"source":["!ls "],"execution_count":15,"outputs":[{"output_type":"stream","text":["calc_metrics.py  docker_run.sh\tmetrics       render.py\t\t  training\n","cmd\t\t docs\t\tMUNCH.zip     SG2_T2_colab.ipynb  training_runs\n","datasets\t downloads\tpretrained    SG2_T_colab.ipynb   train.py\n","dataset_tool.py  generate.py\tprojector.py  SG3_T.ipynb\n","dnnlib\t\t legacy.py\t__pycache__   style_mixing.py\n","Dockerfile\t LICENSE.txt\tREADME.md     torch_utils\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dy-FFW8FpLho","executionInfo":{"status":"ok","timestamp":1619757245136,"user_tz":-540,"elapsed":2366,"user":{"displayName":"Pado파도","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK5icpssdxim4CNM813_k0CxYcl_syRlrFhRSc=s64","userId":"07447657114463313189"}},"outputId":"0aa309c4-adef-40a2-9b36-5c2f7996fce3"},"source":["!python dataset_tool.py --source=m3 --dest=m3.zip  --width=512 --height=512"],"execution_count":19,"outputs":[{"output_type":"stream","text":["100% 65/65 [00:01<00:00, 44.89it/s]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QwcuKnoJjy6N"},"source":[" transfer learning 기존 네트워크사용해서 !!!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f7xPM9qLjsqq","outputId":"91e1cecb-23ba-4873-a454-7f29f859e35a"},"source":["!python train.py --outdir=training_runs --data=m3.zip --resume=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/afhqwild.pkl  --gpus=1 --mirror=1 "],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Training options:\n","{\n","  \"num_gpus\": 1,\n","  \"image_snapshot_ticks\": 50,\n","  \"network_snapshot_ticks\": 50,\n","  \"metrics\": [\n","    \"fid50k_full\"\n","  ],\n","  \"random_seed\": 0,\n","  \"training_set_kwargs\": {\n","    \"class_name\": \"training.dataset.ImageFolderDataset\",\n","    \"path\": \"m3.zip\",\n","    \"use_labels\": false,\n","    \"max_size\": 65,\n","    \"xflip\": true,\n","    \"resolution\": 512\n","  },\n","  \"data_loader_kwargs\": {\n","    \"pin_memory\": true,\n","    \"num_workers\": 3,\n","    \"prefetch_factor\": 2\n","  },\n","  \"G_kwargs\": {\n","    \"class_name\": \"training.networks.Generator\",\n","    \"z_dim\": 512,\n","    \"w_dim\": 512,\n","    \"mapping_kwargs\": {\n","      \"num_layers\": 2\n","    },\n","    \"synthesis_kwargs\": {\n","      \"channel_base\": 32768,\n","      \"channel_max\": 512,\n","      \"num_fp16_res\": 4,\n","      \"conv_clamp\": 256\n","    }\n","  },\n","  \"D_kwargs\": {\n","    \"class_name\": \"training.networks.Discriminator\",\n","    \"block_kwargs\": {},\n","    \"mapping_kwargs\": {},\n","    \"epilogue_kwargs\": {\n","      \"mbstd_group_size\": 4\n","    },\n","    \"channel_base\": 32768,\n","    \"channel_max\": 512,\n","    \"num_fp16_res\": 4,\n","    \"conv_clamp\": 256\n","  },\n","  \"G_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.0025,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"D_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.0025,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"loss_kwargs\": {\n","    \"class_name\": \"training.loss.StyleGAN2Loss\",\n","    \"r1_gamma\": 6.5536\n","  },\n","  \"total_kimg\": 25000,\n","  \"batch_size\": 8,\n","  \"batch_gpu\": 8,\n","  \"ema_kimg\": 2.5,\n","  \"ema_rampup\": null,\n","  \"ada_target\": 0.6,\n","  \"augment_kwargs\": {\n","    \"class_name\": \"training.augment.AugmentPipe\",\n","    \"xflip\": 1,\n","    \"rotate90\": 1,\n","    \"xint\": 1,\n","    \"scale\": 1,\n","    \"rotate\": 1,\n","    \"aniso\": 1,\n","    \"xfrac\": 1,\n","    \"brightness\": 1,\n","    \"contrast\": 1,\n","    \"lumaflip\": 1,\n","    \"hue\": 1,\n","    \"saturation\": 1\n","  },\n","  \"resume_pkl\": \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/afhqwild.pkl\",\n","  \"ada_kimg\": 100,\n","  \"run_dir\": \"training_runs/00000-m3-mirror-auto1-resumecustom\"\n","}\n","\n","Output directory:   training_runs/00000-m3-mirror-auto1-resumecustom\n","Training data:      m3.zip\n","Training duration:  25000 kimg\n","Number of GPUs:     1\n","Number of images:   65\n","Image resolution:   512\n","Conditional model:  False\n","Dataset x-flips:    True\n","\n","Creating output directory...\n","Launching processes...\n","Loading training set...\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","Num images:  130\n","Image shape: [3, 512, 512]\n","Label shape: [0]\n","\n","Constructing networks...\n","Resuming from \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/afhqwild.pkl\"\n","Downloading https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/afhqwild.pkl ... done\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n","\n","Generator             Parameters  Buffers  Output shape        Datatype\n","---                   ---         ---      ---                 ---     \n","mapping.fc0           262656      -        [8, 512]            float32 \n","mapping.fc1           262656      -        [8, 512]            float32 \n","mapping               -           512      [8, 16, 512]        float32 \n","synthesis.b4.conv1    2622465     32       [8, 512, 4, 4]      float32 \n","synthesis.b4.torgb    264195      -        [8, 3, 4, 4]        float32 \n","synthesis.b4:0        8192        16       [8, 512, 4, 4]      float32 \n","synthesis.b4:1        -           -        [8, 512, 4, 4]      float32 \n","synthesis.b8.conv0    2622465     80       [8, 512, 8, 8]      float32 \n","synthesis.b8.conv1    2622465     80       [8, 512, 8, 8]      float32 \n","synthesis.b8.torgb    264195      -        [8, 3, 8, 8]        float32 \n","synthesis.b8:0        -           16       [8, 512, 8, 8]      float32 \n","synthesis.b8:1        -           -        [8, 512, 8, 8]      float32 \n","synthesis.b16.conv0   2622465     272      [8, 512, 16, 16]    float32 \n","synthesis.b16.conv1   2622465     272      [8, 512, 16, 16]    float32 \n","synthesis.b16.torgb   264195      -        [8, 3, 16, 16]      float32 \n","synthesis.b16:0       -           16       [8, 512, 16, 16]    float32 \n","synthesis.b16:1       -           -        [8, 512, 16, 16]    float32 \n","synthesis.b32.conv0   2622465     1040     [8, 512, 32, 32]    float32 \n","synthesis.b32.conv1   2622465     1040     [8, 512, 32, 32]    float32 \n","synthesis.b32.torgb   264195      -        [8, 3, 32, 32]      float32 \n","synthesis.b32:0       -           16       [8, 512, 32, 32]    float32 \n","synthesis.b32:1       -           -        [8, 512, 32, 32]    float32 \n","synthesis.b64.conv0   2622465     4112     [8, 512, 64, 64]    float16 \n","synthesis.b64.conv1   2622465     4112     [8, 512, 64, 64]    float16 \n","synthesis.b64.torgb   264195      -        [8, 3, 64, 64]      float16 \n","synthesis.b64:0       -           16       [8, 512, 64, 64]    float16 \n","synthesis.b64:1       -           -        [8, 512, 64, 64]    float32 \n","synthesis.b128.conv0  1442561     16400    [8, 256, 128, 128]  float16 \n","synthesis.b128.conv1  721409      16400    [8, 256, 128, 128]  float16 \n","synthesis.b128.torgb  132099      -        [8, 3, 128, 128]    float16 \n","synthesis.b128:0      -           16       [8, 256, 128, 128]  float16 \n","synthesis.b128:1      -           -        [8, 256, 128, 128]  float32 \n","synthesis.b256.conv0  426369      65552    [8, 128, 256, 256]  float16 \n","synthesis.b256.conv1  213249      65552    [8, 128, 256, 256]  float16 \n","synthesis.b256.torgb  66051       -        [8, 3, 256, 256]    float16 \n","synthesis.b256:0      -           16       [8, 128, 256, 256]  float16 \n","synthesis.b256:1      -           -        [8, 128, 256, 256]  float32 \n","synthesis.b512.conv0  139457      262160   [8, 64, 512, 512]   float16 \n","synthesis.b512.conv1  69761       262160   [8, 64, 512, 512]   float16 \n","synthesis.b512.torgb  33027       -        [8, 3, 512, 512]    float16 \n","synthesis.b512:0      -           16       [8, 64, 512, 512]   float16 \n","synthesis.b512:1      -           -        [8, 64, 512, 512]   float32 \n","---                   ---         ---      ---                 ---     \n","Total                 28700647    699904   -                   -       \n","\n","\n","Discriminator  Parameters  Buffers  Output shape        Datatype\n","---            ---         ---      ---                 ---     \n","b512.fromrgb   256         16       [8, 64, 512, 512]   float16 \n","b512.skip      8192        16       [8, 128, 256, 256]  float16 \n","b512.conv0     36928       16       [8, 64, 512, 512]   float16 \n","b512.conv1     73856       16       [8, 128, 256, 256]  float16 \n","b512           -           16       [8, 128, 256, 256]  float16 \n","b256.skip      32768       16       [8, 256, 128, 128]  float16 \n","b256.conv0     147584      16       [8, 128, 256, 256]  float16 \n","b256.conv1     295168      16       [8, 256, 128, 128]  float16 \n","b256           -           16       [8, 256, 128, 128]  float16 \n","b128.skip      131072      16       [8, 512, 64, 64]    float16 \n","b128.conv0     590080      16       [8, 256, 128, 128]  float16 \n","b128.conv1     1180160     16       [8, 512, 64, 64]    float16 \n","b128           -           16       [8, 512, 64, 64]    float16 \n","b64.skip       262144      16       [8, 512, 32, 32]    float16 \n","b64.conv0      2359808     16       [8, 512, 64, 64]    float16 \n","b64.conv1      2359808     16       [8, 512, 32, 32]    float16 \n","b64            -           16       [8, 512, 32, 32]    float16 \n","b32.skip       262144      16       [8, 512, 16, 16]    float32 \n","b32.conv0      2359808     16       [8, 512, 32, 32]    float32 \n","b32.conv1      2359808     16       [8, 512, 16, 16]    float32 \n","b32            -           16       [8, 512, 16, 16]    float32 \n","b16.skip       262144      16       [8, 512, 8, 8]      float32 \n","b16.conv0      2359808     16       [8, 512, 16, 16]    float32 \n","b16.conv1      2359808     16       [8, 512, 8, 8]      float32 \n","b16            -           16       [8, 512, 8, 8]      float32 \n","b8.skip        262144      16       [8, 512, 4, 4]      float32 \n","b8.conv0       2359808     16       [8, 512, 8, 8]      float32 \n","b8.conv1       2359808     16       [8, 512, 4, 4]      float32 \n","b8             -           16       [8, 512, 4, 4]      float32 \n","b4.mbstd       -           -        [8, 513, 4, 4]      float32 \n","b4.conv        2364416     16       [8, 512, 4, 4]      float32 \n","b4.fc          4194816     -        [8, 512]            float32 \n","b4.out         513         -        [8, 1]              float32 \n","---            ---         ---      ---                 ---     \n","Total          28982849    480      -                   -       \n","\n","Setting up augmentation...\n","Distributing across 1 GPUs...\n","Setting up training phases...\n","Exporting sample images...\n","Initializing logs...\n","2021-04-30 04:35:36.801178: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","Training for 25000 kimg...\n","\n","tick 0     kimg 0.0      time 1m 41s       sec/tick 13.7    sec/kimg 1718.63 maintenance 86.8   cpumem 5.29   gpumem 13.07  augment 0.000\n","Evaluating metrics...\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","{\"results\": {\"fid50k_full\": 341.6292449548588}, \"metric\": \"fid50k_full\", \"total_time\": 1466.655036687851, \"total_time_str\": \"24m 27s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000000.pkl\", \"timestamp\": 1619758835.6132026}\n","tick 1     kimg 4.0      time 43m 16s      sec/tick 1011.3  sec/kimg 252.82  maintenance 1483.9 cpumem 5.38   gpumem 7.69   augment 0.030\n","tick 2     kimg 8.0      time 1h 00m 08s   sec/tick 1011.9  sec/kimg 252.98  maintenance 0.3    cpumem 5.38   gpumem 7.68   augment 0.064\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ImzBAMGfj6qt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619756858816,"user_tz":-540,"elapsed":945,"user":{"displayName":"Pado파도","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK5icpssdxim4CNM813_k0CxYcl_syRlrFhRSc=s64","userId":"07447657114463313189"}},"outputId":"b2703b10-802a-40be-8e4d-8dee6e684918"},"source":["!ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v-a6FELFIgmx","executionInfo":{"status":"ok","timestamp":1619741854078,"user_tz":-540,"elapsed":1867,"user":{"displayName":"SHOT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5vvPwI0wxV86goecWvaz89UBqVzkx1WnnD5ovSU8=s64","userId":"16892596591524153853"}},"outputId":"599c2deb-b2b3-417f-80c6-dd352f8bd9aa"},"source":["!python dataset_tool.py --source=m3 --dest=m3.zip  --width=256 --height=512"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100% 65/65 [00:00<00:00, 67.87it/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S4kG4rFcJtps","executionInfo":{"status":"ok","timestamp":1619750672721,"user_tz":-540,"elapsed":292,"user":{"displayName":"Pado파도","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK5icpssdxim4CNM813_k0CxYcl_syRlrFhRSc=s64","userId":"07447657114463313189"}},"outputId":"4ca0aab2-acf8-4791-c75f-b51ec5117cec"},"source":["!python train.py --outdir=training_runs --data=m3.zip --gpus=1\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 17 leaked semaphores to clean up at shutdown\n","  len(cache))\n","^C\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"maP-aJUNkth-"},"source":["train 훈련 ~~~~\n"]},{"cell_type":"code","metadata":{"id":"XLghfnCsjrc9"},"source":["dataset = 'degas' #@param {type:\"string\"}\n","resume_pkl = 'https://drive.google.com/file/d/1yyEUSPwaGqudWldiWTMu1nkSRNbEush7/view?usp=sharing' #@param {type:\"string\"}\n","lod_kimg = 30 #@param {type:\"integer\"}\n","\n","%run train.py --data $dataset --resume $resume_pkl --lod_kimg $lod_kimg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cZkcJ58P97Ls"},"source":["## Dataset Preparation\n","\n","Upload a .zip of square images to the `datasets` folder. Previously you had to convert your model to .tfrecords. That’s no longer needed :)"]},{"cell_type":"markdown","metadata":{"id":"5B-h6FpB9FaK"},"source":["## Train model"]},{"cell_type":"markdown","metadata":{"id":"bNc-3wTO-MUd"},"source":["Below are a series of variables you need to set to run the training. You probably won’t need to touch most of them.\n","\n","* `dataset_path`: this is the path to your .zip file\n","* `resume_from`: if you’re starting a new dataset I recommend `'ffhq1024'` or `'./pretrained/wikiart.pkl'`\n","* `mirror_x` and `mirror_y`: Allow the dataset to use horizontal or vertical mirroring."]},{"cell_type":"code","metadata":{"id":"EL-M7WnnfMDI"},"source":["!python train.py --gpus=1 --cfg=$config --metrics=None --outdir=./results --data=$dataset_path --snap=$snapshot_count --resume=$resume_from --augpipe=$augs --initstrength=$aug_strength --gamma=$gamma_value --mirror=$mirror_x --mirrory=False --nkimg=$train_count"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RgvSvfyi_R_-"},"source":["### Resume Training\n","\n","Once Colab has shutdown, you’ll need to resume your training. Reset the variables above, particularly the `resume_from` and `aug_strength` settings.\n","\n","1. Point `resume_from` to the last .pkl you trained (you’ll find these in the `results` folder)\n","2. Update `aug_strength` to match the augment value of the last pkl file. Often you’ll see this in the console, but you may need to look at the `log.txt`. Updating this makes sure training stays as stable as possible.\n","3. You may want to update `train_count` to keep track of your training progress.\n","\n","Once all of this has been reset, run that variable cell and the training command cell after it."]},{"cell_type":"markdown","metadata":{"id":"VznRirOE5ENI"},"source":["## Convert Legacy Model\n","\n","If you have an older version of a model (Tensorflow based StyleGAN, or Runway downloaded .pkl file) you’ll need to convert to the newest version. If you’ve trained in this notebook you do **not** need to use this cell.\n","\n","`--source`: path to model that you want to convert\n","\n","`--dest`: path and file name to convert to."]},{"cell_type":"code","metadata":{"id":"CzkP-Rww5Np9"},"source":["!python legacy.py --source=/content/drive/MyDrive/runway.pkl --dest=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/runway.pkl"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L6EtrPqL9ILk"},"source":["## Testing/Inference\n","\n","Also known as \"Inference\", \"Evaluation\" or \"Testing\" the model. This is the process of usinng your trained model to generate new material, usually images or videos."]},{"cell_type":"markdown","metadata":{"id":"mYdyfH0O8In_"},"source":["### Generate Single Images\n","\n","`--network`: Make sure the `--network` argument points to your .pkl file. (My preferred method is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that into the argument after the `=` sign).\n","\n","`--seeds`: This allows you to choose random seeds from the model. Remember that our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation.\n","\n","`--truncation`: Truncation, well, truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. The smaller the number the more realistic your images should appear, but this will also affect diversity. Most people choose between 0.5 and 1.0, but technically it's infinite. \n"]},{"cell_type":"code","metadata":{"id":"VYRXenMoZSHf"},"source":["!python generate.py --outdir=/content/out/images/ --trunc=0.8 --seeds=0-499 --network=/content/drive/MyDrive/network-snapshot-008720.pkl"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VjOTCWVonoVL"},"source":["### Truncation Traversal\n","\n","Below you can take one seed and look at the changes to it across any truncation amount. -1 to 1 will be pretty realistic images, but the further out you get the weirder it gets.\n","\n","#### Options \n","`--network`: Again, this should be the path to your .pkl file.\n","\n","`--seeds`: Pass this only one seed. Pick a favorite from your generated images.\n","\n","`--start`: Starting truncation value.\n","\n","`--stop`: Stopping truncation value. This should be larger than the start value. (Will probably break if its not).\n","\n","`--increment`: How much each frame should increment the truncation value. Make this really small if you want a long, slow interpolation. (stop-start/increment=total frames)\n"]},{"cell_type":"code","metadata":{"id":"nyzdGr7OnrMG"},"source":["!python generate.py --process=\"truncation\" --outdir=/content/out/trunc-trav-3/ --start=-0.8 --stop=2.8 --increment=0.02 --seeds=470 --network=/content/drive/MyDrive/stylegan2-transfer-models/mixed6k-network-snapshot-016470.pkl"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OSzj0igO8Lfu"},"source":["### Interpolations\n","\n","Interpolation is the process of generating very small changes to a vector in order to make it appear animated from frame to frame.\n","\n","We’ll look at different examples of interpolation below.\n","\n","#### Options\n","\n","`--network`: path to your .pkl file\n","\n","`--interpolation`: Walk type defines the type of interpolation you want. In some cases it can also specify whether you want the z space or the w space.\n","\n","`--frames`: How many frames you want to produce. Use this to manage the length of your video.\n","\n","`--trunc`: truncation value"]},{"cell_type":"markdown","metadata":{"id":"OJSqafIzNwhx"},"source":["#### Linear Interpolation"]},{"cell_type":"code","metadata":{"id":"sqkiskly8S5_"},"source":["!python generate.py --outdir=/content/out/video1-w-0.5/ --space=\"z\" --trunc=0.5 --process=\"interpolation\" --seeds=463,470 --network=/content/drive/MyDrive/stylegan2-transfer-models/mixed6k-network-snapshot-016470.pkl --frames=48"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DCUEV3aO8s_X"},"source":["!python generate.py --outdir=out/video1-w/ --space=\"w\" --trunc=1 --process=\"interpolation\" --seeds=85,265,297,849 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pmKbwZDD8gjM"},"source":["!zip -r vid1.zip /content/out/video1-w-0.5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yi3d7xzpN2Uj"},"source":["#### Slerp Interpolation\n","\n","This gets a little heady, but technically linear interpolations are not the best in high-dimensional GANs. [This github link](https://github.com/soumith/dcgan.torch/issues/14) is one of the more popular explanations ad discussions.\n","\n","In reality I do not find a huge difference between linear and spherical interpolations (the difference in z- and w-space is enough in many cases), but I’ve implemented slerp here for anyone interested.\n","\n","Note: Slerp in w space currently isn’t supported. I’m working on it."]},{"cell_type":"code","metadata":{"id":"I0-cUd3fB_kJ"},"source":["!python generate.py --outdir=out/video1/ --trunc=1 --process=\"interpolation\" --interpolation=\"slerp\" --seeds=85,265,297,849 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uP1HsU_CPcF5"},"source":["#### Noise Loop\n","\n","If you want to just make a random but fun interpolation of your model the noise loop is the way to go. It creates a random path thru the z space to show you a diverse set of images.\n","\n","`--interpolation=\"noiseloop\"`: set this to use the noise loop funtion\n","\n","`--diameter`: This controls how \"wide\" the loop is. Make it smaller to show a less diverse range of samples. Make it larger to cover a lot of samples. This plus `--frames` can help determine how fast the video feels.\n","\n","`--random_seed`: this allows you to change your starting place in the z space. Note: this value has nothing to do with the seeds you use to generate images. It just allows you to randomize your start point (and if you want to return to it you can use the same seed multiple times).\n","\n","Noise loops currently only work in z space."]},{"cell_type":"code","metadata":{"id":"gfR6DhfvN8b_"},"source":["!python generate.py --outdir=out/video-noiseloop-0.9d/ --trunc=0.8 --process=\"interpolation\" --interpolation=\"noiseloop\" --diameter=0.9 --random_seed=100 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PkKFb-4CedOq"},"source":["#### Circular Loop\n","\n","The noise loop is, well, noisy. This circular loop will feel much more even, while still providing a random loop.\n","\n","I recommend using a higher `--diameter` value than you do with noise loops. Something between `50.0` and `500.0` alongside `--frames` can help control speed and diversity."]},{"cell_type":"code","metadata":{"id":"Ao62za9_QfOF"},"source":["!python generate.py --outdir=out/video-circularloop/ --trunc=1 --process=\"interpolation\" --interpolation=\"circularloop\" --diameter=800.00 --frames=720 --random_seed=90 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T8Ld_ozNJCOn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qz-fVtzyAHg1"},"source":["## Projection"]},{"cell_type":"markdown","metadata":{"id":"ez7tXSpCA_zh"},"source":["\n","\n","*   `--target`: this is a path to the image file that you want to \"find\" in your model. This image must be the exact same size as your model.\n","*   `--num-steps`: how many iterations the projctor should run for. Lower will mean less steps and less likelihood of a good projection. Higher will take longer but will likely produce better images.\n","\n"]},{"cell_type":"code","metadata":{"id":"p84CtZUGAKnR"},"source":["!python projector.py --help"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"80YTcjIQARWh"},"source":["!python projector.py --network=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00023-chin-morris-mirror-11gb-gpu-gamma50-bg-resumecustom/network-snapshot-000304.pkl --outdir=/content/projector/ --target=/content/img005421_0.png --num-steps=200 --seed=0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hAxADbdpHHib"},"source":["### Peter Baylies’ Projector"]},{"cell_type":"code","metadata":{"id":"iwS_ey9QF-nk"},"source":["!python /content/stylegan2-ada-pytorch/pbaylies_projector.py --help"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-yj06MAABoLe"},"source":["!python /content/stylegan2-ada-pytorch/pbaylies_projector.py --network=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00023-chin-morris-mirror-11gb-gpu-gamma50-bg-resumecustom/network-snapshot-000304.pkl --outdir=/content/projector-no-clip/ --target-image=/content/img005421_0.png --num-steps=200 --use-clip=False --seed=0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qywlaS5pgzyH"},"source":["## Combine NPZ files together"]},{"cell_type":"code","metadata":{"id":"M2VooqrNfIpw"},"source":["!python combine_npz.py --outdir=/content/npz --npzs='/content/projector/projected_w.npz,/content/projector-no-clip/projected_w.npz'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uIqgl5nIHwpp"},"source":["!python generate.py --help"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4cgezYN8Dsyh"},"source":["!python generate.py --process=interpolation --interpolation=linear --space=w --network=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00023-chin-morris-mirror-11gb-gpu-gamma50-bg-resumecustom/network-snapshot-000304.pkl --outdir=/content/test/ --projected-w=/content/npz/combined.npz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lF7RCnSAsWrq"},"source":["## Feature Extraction using Closed Form Factorization\n","\n","Feature Extraction is the process of finding “human readable” vectors in a StyleGAN model. For example, let’s say you wanted to find a vector that could open or close a mouth in a face model.\n","\n","The feature extractor tries to automate the procss of finding important vectors in your model.\n","\n","`--ckpt`: This is the path to your .pkl file. In other places its called `--network` (It’s a long story for why its name changed here)\n","`--out`: path to save your output feature vector file. The file name must end in `.pt`!"]},{"cell_type":"code","metadata":{"id":"1Hek6TFZCKD-"},"source":["!python closed_form_factorization.py --out=/content/ladies-black-cff.pt --ckpt=/content/drive/MyDrive/network-snapshot-008720.pkl"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WxLgeNeJRqFh"},"source":["Once this cell is finished you’ll want to save that `.pt` file somewhere for reuse.\n","\n","This process just created the vctor values, but we need to test it on some seed values to determine what each vector actually changes. The `apply_factor.py` script does this.\n","\n","Arguments to try:\n","\n","\n","*   `-i`: This stands for index. By default, the cell above will produce 512 vectors, so `-i` can be any value from 0 to 511. I recommend starting with a higher value.\n","*   `-d`: This stands for degrees. This means how much change you want to see along th vector. I recommend a value between 5 and 10 to start with.\n","*   `--seeds`: You know what these are by now right? :)\n","*   `--ckpt`: path to your .pkl file\n","*   `--video`: adding this to your argument will produce a video that animates your seeds along the vector path. I find it much easier to figure out what’s changing with an animation.\n","*   `--output`: where to save the images/video\n","\n","Lastly you need to add the path to the `.pt` file you made in th above cell. It’s weird, but you don’t need to add any arguments bfore it, just make sure its after `apply_factor.pt`\n","\n"]},{"cell_type":"code","metadata":{"id":"dEDSl2VpCSJL"},"source":["!python apply_factor.py -i 0 -d 10 --seeds 5,10 --ckpt /content/drive/MyDrive/network-snapshot-008720.pkl /content/ladies-black-cff.pt --output /content/cff-vid/ --video"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mzwhrjGlTMZ3"},"source":["That just produced images or video for a single vector, but there are 511 more! To generate every vector, you can uuse the cell below. Update any arguments you want, but don’t touch the `-i {i}` part.\n","\n","**Warning:** This takes a long time, especially if you have more than one seed value (pro tip: don’t usee more than one seed value)! Also, this will take up a good amount of space in Google Drive. You’ve been warned!"]},{"cell_type":"code","metadata":{"id":"6aFj6mcKDmqk"},"source":["for i in range(512):\n","  !python apply_factor.py -i {i} -d 10 --seeds 177 --ckpt /content/drive/MyDrive/network-snapshot-008720.pkl /content/ladies-black-cff.pt --output /content/drive/MyDrive/ladiesblack-cff-17/ --video #--out_prefix 'ladiesblack-factor-{i}'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UVfmNV5JEcdp"},"source":[""],"execution_count":null,"outputs":[]}]}